import pandas as pd
from io import BytesIO
from bson import ObjectId
from fastapi import APIRouter, UploadFile, File, HTTPException, status, Depends, Query
from fastapi.responses import JSONResponse, StreamingResponse
from datetime import datetime, timedelta, timezone
from pymongo import ASCENDING
from pymongo.errors import PyMongoError
from ..database import get_database, serialize_mongo_document
import os
import requests
import json
import time, io
import gzip
import re
from typing import Optional, Dict, Any, List
from dotenv import load_dotenv
import logging
from .amazon_vc import router as amazon_vendor_router

# --- Configuration ---
# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ENV
load_dotenv()

SKU_COLLECTION = "amazon_sku_mapping"
SALES_COLLECTION = "amazon_sales_traffic"
INVENTORY_COLLECTION = "amazon_ledger"
FBA_RETURNS_COLLECTION = "amazon_fba_returns"
SC_RETURNS_COLLECTION = "amazon_seller_central_returns"

router = APIRouter()

router.include_router(amazon_vendor_router, prefix="/vendor")

# SP API Configuration
REFRESH_TOKEN = os.getenv("SP_REFRESH_TOKEN")
GRANT_TYPE = os.getenv("SP_GRANT_TYPE", "refresh_token")
CLIENT_ID = os.getenv("SP_CLIENT_ID")
CLIENT_SECRET = os.getenv("SP_CLIENT_SECRET")
LOGIN_URL = os.getenv("SP_LOGIN_URL", "https://api.amazon.com/auth/o2/token")
MARKETPLACE_ID = os.getenv("SP_MARKETPLACE_ID", "A21TJRUUN4KGV")  # IN marketplace
SP_API_BASE_URL = os.getenv(
    "SP_API_BASE_URL", "https://sellingpartnerapi-na.amazon.com"
)

# Global variable to store access token and expiry
_sc_access_token = None
_sc_token_expires_at = None


def get_amazon_token() -> str:
    """
    Get a new access token from Amazon SP API using refresh token
    """
    global _sc_access_token, _sc_token_expires_at

    # Check if current token is still valid (with 5 minute buffer)
    if (
        _sc_access_token
        and _sc_token_expires_at
        and datetime.now() < (_sc_token_expires_at - timedelta(minutes=5))
    ):
        return _sc_access_token

    try:
        payload = {
            "grant_type": GRANT_TYPE,
            "refresh_token": REFRESH_TOKEN,
            "client_id": CLIENT_ID,
            "client_secret": CLIENT_SECRET,
        }

        headers = {"Content-Type": "application/x-www-form-urlencoded"}

        response = requests.post(LOGIN_URL, data=payload, headers=headers)
        response.raise_for_status()

        token_data = response.json()
        _sc_access_token = token_data.get("access_token")
        expires_in = token_data.get("expires_in", 3600)  # Default 1 hour
        _sc_token_expires_at = datetime.now() + timedelta(seconds=expires_in)

        logger.info("Successfully obtained new Amazon SP API token")
        return _sc_access_token

    except requests.exceptions.RequestException as e:
        logger.error(f"Error getting Amazon token: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to authenticate with Amazon SP API: {str(e)}",
        )
    except Exception as e:
        logger.error(f"Unexpected error getting Amazon token: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An unexpected error occurred during authentication: {str(e)}",
        )


def make_sp_api_request(
    endpoint: str, method: str = "GET", params: Dict = None, data: Dict = None
) -> Dict:
    """
    Make a request to Amazon SP API with proper authentication
    """
    access_token = get_amazon_token()

    headers = {"x-amz-access-token": access_token, "Content-Type": "application/json"}

    url = f"{SP_API_BASE_URL}{endpoint}"

    try:
        if method.upper() == "GET":
            response = requests.get(url, headers=headers, params=params)
        elif method.upper() == "POST":
            response = requests.post(url, headers=headers, json=data, params=params)
        else:
            raise ValueError(f"Unsupported HTTP method: {method}")

        response.raise_for_status()
        return response.json()

    except requests.exceptions.RequestException as e:
        logger.error(f"SP API request failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Amazon SP API request failed: {str(e)}",
        )


def create_report(
    report_type: str,
    start_date: str,
    end_date: str,
    marketplace_ids: List[str] = None,
    report_options: Dict = None,
) -> str:
    """
    Create a report request and return the report document ID
    """
    if marketplace_ids is None:
        marketplace_ids = [MARKETPLACE_ID]

    endpoint = "/reports/2021-06-30/reports"

    report_data = {
        "reportType": report_type,
        "dataStartTime": start_date,
        "dataEndTime": end_date,
        "marketplaceIds": marketplace_ids,
    }

    # Add report options if provided (for ledger reports)
    if report_options:
        report_data["reportOptions"] = report_options

    response = make_sp_api_request(endpoint, method="POST", data=report_data)
    return response.get("reportId")


def get_report_status(report_document_id: str) -> Dict:
    """
    Check the status of a report
    """
    endpoint = f"/reports/2021-06-30/reports/{report_document_id}"
    return make_sp_api_request(endpoint)


def download_report_data(report_document_id: str, is_gzipped: bool = False) -> str:
    """
    Download report data using the document ID
    """
    endpoint = f"/reports/2021-06-30/documents/{report_document_id}"
    document_info = make_sp_api_request(endpoint)

    # Get the download URL
    download_url = document_info.get("url")
    if not download_url:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="No download URL found in report document",
        )

    # Download the actual report data
    response = requests.get(download_url)
    response.raise_for_status()

    # Handle gzipped content (for ledger reports)
    if is_gzipped:
        try:
            return gzip.decompress(response.content).decode("utf-8")
        except Exception as e:
            logger.error(f"Error decompressing gzipped content: {e}")
            # Fallback to regular content if not actually gzipped
            return response.text

    return response.text


def get_amazon_sales_traffic_data(
    start_date: str, end_date: str, db: Any, marketplace_ids: List[str] = None
) -> List[Dict]:
    """
    Fetch sales and traffic data from Amazon SP API (ASIN-wise daily data)
    """
    if marketplace_ids is None:
        marketplace_ids = [MARKETPLACE_ID]

    try:
        # Create sales and traffic report
        report_id = create_report(
            report_type="GET_SALES_AND_TRAFFIC_REPORT",
            start_date=start_date,
            end_date=end_date,
            marketplace_ids=marketplace_ids,
        )

        # Add validation for report_id
        if not report_id:
            raise ValueError("Failed to create report - no report ID returned")

        # Wait for report to be ready (with timeout)
        max_wait_time = 300  # 5 minutes
        wait_time = 0

        while wait_time < max_wait_time:
            report_status = get_report_status(report_id)

            # Add validation for report_status
            if not report_status:
                raise ValueError("Failed to get report status - empty response")

            processing_status = report_status.get("processingStatus")
            report_document_id = report_status.get("reportDocumentId")

            if processing_status == "DONE":
                break
            elif processing_status == "FATAL":
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="Sales and traffic report processing failed",
                )

            time.sleep(10)
            wait_time += 10

        if wait_time >= max_wait_time:
            raise HTTPException(
                status_code=status.HTTP_408_REQUEST_TIMEOUT,
                detail="Sales and traffic report processing timed out",
            )

        # Validate report_document_id before downloading
        if not report_document_id:
            raise ValueError("No report document ID available")

        # Download and parse report data (JSON format)
        report_data = download_report_data(report_document_id, is_gzipped=True)

        # Add validation for report_data
        if not report_data:
            raise ValueError("Downloaded report data is empty")

        # Parse JSON data with better error handling
        try:
            json_data = json.loads(report_data)
        except json.JSONDecodeError as json_error:
            raise ValueError(f"Failed to parse JSON report data: {json_error}")

        # Validate json_data structure
        if not isinstance(json_data, dict):
            raise ValueError("Invalid JSON structure - expected dictionary")

        # Extract ASIN-wise daily data
        sales_traffic_data = []

        # Process salesAndTrafficByAsin data
        if "salesAndTrafficByAsin" in json_data:
            asin_data_list = json_data["salesAndTrafficByAsin"]

            if not isinstance(asin_data_list, list):
                raise ValueError("salesAndTrafficByAsin is not a list")

            for asin_data in asin_data_list:
                asin = asin_data["parentAsin"]
                asin_data["date"] = datetime.fromisoformat(start_date)
                asin_data["created_at"] = datetime.now()
                result = db[SALES_COLLECTION].find_one(
                    {"parentAsin": asin, "date": asin_data["date"]}
                )
                if not result:
                    sales_traffic_data.append(asin_data)
        else:
            logger.warning("No 'salesAndTrafficByAsin' data found in report")

        return sales_traffic_data

    except HTTPException:
        # Re-raise HTTPExceptions as-is
        raise
    except ValueError as ve:
        logger.error(f"Validation error in sales and traffic data: {ve}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Data validation error: {str(ve)}",
        )
    except Exception as e:
        logger.error(f"Unexpected error fetching sales and traffic data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch sales and traffic data: {str(e)}",
        )


def get_amazon_ledger_data(
    start_date: str,
    end_date: str,
    db: Any,
    marketplace_ids: List[str] = None,
    aggregate_by_location: str = "COUNTRY",
    aggregated_by_time_period: str = "DAILY",
) -> List[Dict]:
    logger.info(f"Processing dates - start: {start_date}, end: {end_date}")
    """
    Fetch ledger summary data from Amazon SP API
    """
    if marketplace_ids is None:
        marketplace_ids = [MARKETPLACE_ID]

    try:
        # Report options for ledger data
        report_options = {
            "aggregateByLocation": aggregate_by_location,
            "aggregatedByTimePeriod": aggregated_by_time_period,
        }

        # Create ledger report
        report_id = create_report(
            report_type="GET_LEDGER_SUMMARY_VIEW_DATA",
            start_date=start_date,
            end_date=end_date,
            marketplace_ids=marketplace_ids,
            report_options=report_options,
        )

        # Wait for report to be ready (with timeout)
        max_wait_time = 300  # 5 minutes
        wait_time = 0
        report_document_id = ""
        while wait_time < max_wait_time:
            report_status = get_report_status(report_id)
            processing_status = report_status.get("processingStatus")
            report_document_id = report_status.get("reportDocumentId")

            if processing_status == "DONE":
                break
            elif processing_status == "FATAL":
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="Ledger report processing failed",
                )

            time.sleep(10)
            wait_time += 10

        if wait_time >= max_wait_time:
            raise HTTPException(
                status_code=status.HTTP_408_REQUEST_TIMEOUT,
                detail="Ledger report processing timed out",
            )

        # Download and parse report data (TSV format, gzipped)
        report_data = download_report_data(report_document_id, is_gzipped=True)

        # Convert TSV data to list of dictionaries
        df = pd.read_csv(BytesIO(report_data.encode("utf-8")), sep="\t")
        ledger_data = df.to_dict("records")
        # Add metadata
        normalized_data = []
        for record in ledger_data:
            normalized_record = {
                key.lower().replace(" ", "_"): value for key, value in record.items()
            }
            normalized_record["date"] = datetime.fromisoformat(start_date)
            normalized_record["created_at"] = datetime.now()
            normalized_record["report_type"] = "ledger"
            result = db.amazon_ledger.find_one(
                {
                    "date": normalized_record["date"],
                    "asin": normalized_record["asin"],
                    "location": normalized_record["location"],
                    "ending_warehouse_balance": normalized_record[
                        "ending_warehouse_balance"
                    ],
                }
            )
            if not result:
                normalized_data.append(normalized_record)

        return normalized_data

    except Exception as e:
        logger.error(f"Error fetching ledger data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch ledger data: {str(e)}",
        )


def get_amazon_inventory_data() -> List[Dict]:
    """
    Fetch inventory data from Amazon SP API
    """
    try:
        endpoint = "/fba/inventory/v1/summaries"
        params = {
            "granularityType": "Marketplace",
            "granularityId": MARKETPLACE_ID,
            "marketplaceIds": MARKETPLACE_ID,
        }

        response = make_sp_api_request(endpoint, params=params)
        inventory_summaries = response.get("inventorySummaries", [])

        # Add metadata
        for record in inventory_summaries:
            record["created_at"] = datetime.now()
            record["_report_type"] = "inventory"

        return inventory_summaries

    except Exception as e:
        logger.error(f"Error fetching inventory data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch inventory data: {str(e)}",
        )


async def insert_data_to_db(
    collection_name: str, data: List[Dict], db=Depends(get_database)
) -> int:
    """
    Insert data into MongoDB collection
    """
    try:
        if not data:
            return 0

        collection = db[collection_name]

        # Insert data
        result = collection.insert_many(data)
        logger.info(
            f"Inserted {len(result.inserted_ids)} records into {collection_name}"
        )

        return len(result.inserted_ids)

    except PyMongoError as e:
        logger.error(f"Database error inserting data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Database error: {str(e)}",
        )
    except Exception as e:
        logger.error(f"Unexpected error inserting data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An unexpected error occurred: {str(e)}",
        )


def get_field_mapping_for_amazon_field(amazon_field: str) -> str:
    """
    Dynamically map Amazon field names to our expected field names
    """
    normalized = normalize_field_name(amazon_field)

    # Define mapping rules based on common patterns
    field_mappings = {
        # Order fields
        "order_id": "order_id",
        "orderid": "order_id",
        "amazon_order_id": "order_id",
        "amazonorderid": "order_id",
        # Dates
        "order_date": "order_date",
        "orderdate": "order_date",
        "return_request_date": "return_request_date",
        "returnrequestdate": "return_request_date",
        "return_delivery_date": "return_delivery_date",
        "returndeliverydate": "return_delivery_date",
        # Return information
        "return_request_status": "return_request_status",
        "returnrequeststatus": "return_request_status",
        "return_quantity": "return_quantity",
        "returnquantity": "return_quantity",
        "quantity": "return_quantity",
        "return_reason": "return_reason",
        "returnreason": "return_reason",
        "return_type": "return_type",
        "returntype": "return_type",
        # RMA fields
        "amazon_rma_id": "amazon_rma_id",
        "amazonrmaid": "amazon_rma_id",
        "merchant_rma_id": "merchant_rma_id",
        "merchantrmaid": "merchant_rma_id",
        # Shipping/Label fields
        "label_type": "label_type",
        "labeltype": "label_type",
        "label_cost": "label_cost",
        "labelcost": "label_cost",
        "currency_code": "currency_code",
        "currencycode": "currency_code",
        "return_carrier": "return_carrier",
        "returncarrier": "return_carrier",
        "tracking_id": "tracking_id",
        "trackingid": "tracking_id",
        "label_to_be_paid_by": "label_to_be_paid_by",
        "labeltobepaidby": "label_to_be_paid_by",
        # Claims and prime
        "a_to_z_claim": "a_to_z_claim",
        "atozclaim": "a_to_z_claim",
        "is_prime": "is_prime",
        "isprime": "is_prime",
        # Product fields
        "asin": "asin",
        "merchant_sku": "merchant_sku",
        "merchantsku": "merchant_sku",
        "sku": "merchant_sku",
        "item_name": "item_name",
        "itemname": "item_name",
        "product_name": "item_name",
        "productname": "item_name",
        "category": "category",
        # Policy and resolution
        "in_policy": "in_policy",
        "inpolicy": "in_policy",
        "resolution": "resolution",
        # Financial fields
        "order_amount": "order_amount",
        "orderamount": "order_amount",
        "order_quantity": "order_quantity",
        "orderquantity": "order_quantity",
        "refunded_amount": "refunded_amount",
        "refundedamount": "refunded_amount",
        # Other fields
        "invoice_number": "invoice_number",
        "invoicenumber": "invoice_number",
        "order_item_id": "order_item_id",
        "orderitemid": "order_item_id",
    }

    return field_mappings.get(normalized, normalized)


def normalize_amazon_sc_fields(record: Dict) -> Dict:
    """
    Normalize Amazon SC returns record to our expected structure
    """
    normalized_record = {}

    for amazon_field, value in record.items():
        # Get the mapped field name
        our_field = get_field_mapping_for_amazon_field(amazon_field)

        # Handle null/empty values
        if pd.isna(value) or value == "" or value == " ":
            normalized_record[our_field] = None
        else:
            # Handle special data type conversions
            if our_field in [
                "order_date",
                "return_request_date",
                "return_delivery_date",
            ]:
                try:
                    normalized_record[our_field] = pd.to_datetime(value).to_pydatetime()
                except:
                    normalized_record[our_field] = value
            elif our_field in ["label_cost"]:
                try:
                    normalized_record[our_field] = float(value)
                except:
                    normalized_record[our_field] = value
            elif our_field in ["return_quantity"]:
                try:
                    normalized_record[our_field] = int(float(value))
                except:
                    normalized_record[our_field] = value
            elif our_field == "tracking_id":
                try:
                    # Handle long tracking IDs
                    normalized_record[our_field] = int(float(value))
                except:
                    normalized_record[our_field] = value
            else:
                normalized_record[our_field] = value

    return normalized_record


def normalize_field_name(field_name: str) -> str:
    """
    Convert Amazon field names to our snake_case format
    """
    if not field_name:
        return ""

    # Remove extra spaces and convert to lowercase
    field_name = field_name.strip().lower()

    # Replace spaces, hyphens, and other separators with underscores
    field_name = re.sub(r"[\s\-\.]+", "_", field_name)

    # Remove special characters but keep underscores
    field_name = re.sub(r"[^a-z0-9_]", "", field_name)

    # Remove multiple consecutive underscores
    field_name = re.sub(r"_+", "_", field_name)

    # Remove leading/trailing underscores
    field_name = field_name.strip("_")

    return field_name


def normalize_amazon_fba_fields(record: Dict) -> Dict:
    """
    Normalize Amazon FBA returns record to our expected structure
    """
    normalized_record = {}

    # FBA field mappings
    fba_field_mappings = {
        "return_date": "return_date",
        "returndate": "return_date",
        "order_id": "amazon_order_id",
        "orderid": "amazon_order_id",
        "amazon_order_id": "amazon_order_id",
        "amazonorderid": "amazon_order_id",
        "sku": "sku_code",
        "sku_code": "sku_code",
        "skucode": "sku_code",
        "asin": "asin",
        "fnsku": "fnsku",
        "product_name": "product_name",
        "productname": "product_name",
        "item_name": "product_name",
        "itemname": "product_name",
        "quantity": "quantity",
        "fulfillment_center_id": "fulfillment_center_id",
        "fulfillmentcenterid": "fulfillment_center_id",
        "detailed_disposition": "detailed_disposition",
        "detaileddisposition": "detailed_disposition",
        "reason": "reason",
        "license_plate_number": "license_plate_number",
        "licenseplatenumber": "license_plate_number",
        "customer_comments": "customer_comments",
        "customercomments": "customer_comments",
    }

    for amazon_field, value in record.items():
        # Get normalized field name
        normalized_field = normalize_field_name(amazon_field)

        # Get our target field name
        our_field = fba_field_mappings.get(normalized_field, normalized_field)

        # Handle null/empty values
        if pd.isna(value) or value == "" or value == " ":
            normalized_record[our_field] = None
        else:
            # Handle special data type conversions
            if our_field == "return_date":
                try:
                    normalized_record[our_field] = pd.to_datetime(value).to_pydatetime()
                except:
                    normalized_record[our_field] = value
            elif our_field == "quantity":
                try:
                    normalized_record[our_field] = int(float(value))
                except:
                    normalized_record[our_field] = value
            else:
                normalized_record[our_field] = value

    return normalized_record


def get_amazon_fba_returns_data(
    start_date: str, end_date: str, db: Any, marketplace_ids: List[str] = None
) -> List[Dict]:
    """
    Fetch FBA returns data from Amazon SP API
    """
    if marketplace_ids is None:
        marketplace_ids = [MARKETPLACE_ID]

    try:
        # Create FBA returns report
        report_id = create_report(
            report_type="GET_FBA_FULFILLMENT_CUSTOMER_RETURNS_DATA",
            start_date=start_date,
            end_date=end_date,
            marketplace_ids=marketplace_ids,
        )

        # Add validation for report_id
        if not report_id:
            raise ValueError(
                "Failed to create FBA returns report - no report ID returned"
            )

        # Wait for report to be ready (with timeout)
        max_wait_time = 300  # 5 minutes
        wait_time = 0

        while wait_time < max_wait_time:
            report_status = get_report_status(report_id)

            # Add validation for report_status
            if not report_status:
                raise ValueError(
                    "Failed to get FBA returns report status - empty response"
                )

            processing_status = report_status.get("processingStatus")
            report_document_id = report_status.get("reportDocumentId")

            if processing_status == "DONE":
                break
            elif processing_status == "FATAL":
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="FBA returns report processing failed",
                )

            time.sleep(10)
            wait_time += 10

        if wait_time >= max_wait_time:
            raise HTTPException(
                status_code=status.HTTP_408_REQUEST_TIMEOUT,
                detail="FBA returns report processing timed out",
            )

        # Validate report_document_id before downloading
        if not report_document_id:
            raise ValueError("No FBA returns report document ID available")

        # Download and parse report data (TSV format)
        report_data = download_report_data(report_document_id, is_gzipped=False)

        # Add validation for report_data
        if not report_data:
            raise ValueError("Downloaded FBA returns report data is empty")

        # Convert TSV data to list of dictionaries
        try:
            df = pd.read_csv(io.StringIO(report_data), sep="\t", low_memory=False)
            fba_returns_raw = df.to_dict("records")
        except Exception as parse_error:
            raise ValueError(f"Failed to parse FBA returns TSV data: {parse_error}")

        # Process and normalize data according to your document structure
        normalized_data = []
        for record in fba_returns_raw:
            # Apply dynamic field mapping
            normalized_record = normalize_amazon_fba_fields(record)

            # Check for duplicate using amazon_order_id + sku_code + return_date
            should_insert = True
            if all(
                key in normalized_record and normalized_record[key] is not None
                for key in ["amazon_order_id", "sku_code", "return_date"]
            ):
                duplicate_check = {
                    "amazon_order_id": normalized_record["amazon_order_id"],
                    "sku_code": normalized_record["sku_code"],
                    "return_date": normalized_record["return_date"],
                }

                existing_record = db[FBA_RETURNS_COLLECTION].find_one(duplicate_check)
                if existing_record:
                    should_insert = False
                    logger.debug(
                        f"Skipping duplicate FBA return: {normalized_record['amazon_order_id']} - {normalized_record['sku_code']}"
                    )

            if should_insert:
                normalized_data.append(normalized_record)

        logger.info(
            f"Processed {len(fba_returns_raw)} FBA returns records, {len(normalized_data)} new records to insert"
        )
        return normalized_data

    except HTTPException:
        # Re-raise HTTPExceptions as-is
        raise
    except ValueError as ve:
        logger.error(f"Validation error in FBA returns data: {ve}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"FBA returns data validation error: {str(ve)}",
        )
    except Exception as e:
        logger.error(f"Unexpected error fetching FBA returns data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch FBA returns data: {str(e)}",
        )


def get_amazon_sc_returns_data(
    start_date: str, end_date: str, db: Any, marketplace_ids: List[str] = None
) -> List[Dict]:
    """
    Fetch Seller Central returns data from Amazon SP API
    """
    if marketplace_ids is None:
        marketplace_ids = [MARKETPLACE_ID]

    try:
        # Create Seller Central returns report
        report_id = create_report(
            report_type="GET_FLAT_FILE_RETURNS_DATA_BY_RETURN_DATE",
            start_date=start_date,
            end_date=end_date,
            marketplace_ids=marketplace_ids,
        )

        # Add validation for report_id
        if not report_id:
            raise ValueError(
                "Failed to create SC returns report - no report ID returned"
            )

        # Wait for report to be ready (with timeout)
        max_wait_time = 300  # 5 minutes
        wait_time = 0

        while wait_time < max_wait_time:
            report_status = get_report_status(report_id)

            # Add validation for report_status
            if not report_status:
                raise ValueError(
                    "Failed to get SC returns report status - empty response"
                )

            processing_status = report_status.get("processingStatus")
            report_document_id = report_status.get("reportDocumentId")

            if processing_status == "DONE":
                break
            elif processing_status == "FATAL":
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="SC returns report processing failed",
                )

            time.sleep(10)
            wait_time += 10

        if wait_time >= max_wait_time:
            raise HTTPException(
                status_code=status.HTTP_408_REQUEST_TIMEOUT,
                detail="SC returns report processing timed out",
            )

        # Validate report_document_id before downloading
        if not report_document_id:
            raise ValueError("No SC returns report document ID available")

        # Download and parse report data (TSV format)
        report_data = download_report_data(report_document_id, is_gzipped=False)

        # Add validation for report_data
        if not report_data:
            raise ValueError("Downloaded SC returns report data is empty")

        # Convert TSV data to list of dictionaries
        try:
            df = pd.read_csv(io.StringIO(report_data), sep="\t", low_memory=False)
            sc_returns_raw = df.to_dict("records")
        except Exception as parse_error:
            raise ValueError(f"Failed to parse SC returns TSV data: {parse_error}")

        # Process and normalize data according to your document structure
        normalized_data = []
        for record in sc_returns_raw:
            # Map Amazon column names to your document structure
            normalized_record = {}

            # Apply dynamic field mapping
            normalized_record = normalize_amazon_sc_fields(record)

            # Check for duplicate using order_id + merchant_sku + return_request_date
            should_insert = True
            if all(
                key in normalized_record and normalized_record[key] is not None
                for key in ["order_id", "merchant_sku", "return_request_date"]
            ):
                duplicate_check = {
                    "order_id": normalized_record["order_id"],
                    "merchant_sku": normalized_record["merchant_sku"],
                    "return_request_date": normalized_record["return_request_date"],
                }

                existing_record = db[SC_RETURNS_COLLECTION].find_one(duplicate_check)
                if existing_record:
                    should_insert = False
                    logger.debug(
                        f"Skipping duplicate SC return: {normalized_record['order_id']} - {normalized_record['merchant_sku']}"
                    )

            if should_insert:
                normalized_data.append(normalized_record)

        logger.info(
            f"Processed {len(sc_returns_raw)} SC returns records, {len(normalized_data)} new records to insert"
        )
        return normalized_data

    except HTTPException:
        # Re-raise HTTPExceptions as-is
        raise
    except ValueError as ve:
        logger.error(f"Validation error in SC returns data: {ve}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"SC returns data validation error: {str(ve)}",
        )
    except Exception as e:
        logger.error(f"Unexpected error fetching SC returns data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch SC returns data: {str(e)}",
        )


# API Endpoints


@router.post("/auth/token")
async def refresh_amazon_token():
    """
    Refresh Amazon SP API token
    """
    try:
        token = get_amazon_token()
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "Token refreshed successfully",
                "token_expires_at": (
                    _sc_token_expires_at.isoformat() if _sc_token_expires_at else None
                ),
            },
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e)
        )


@router.post("/sync/sales-traffic")
async def sync_sales_traffic_data(
    start_date: str,
    end_date: str,
    marketplace_ids: Optional[List[str]] = None,
    db=Depends(get_database),
):
    """
    Fetch ASIN-wise daily sales and traffic data from Amazon and insert into database
    Format: YYYY-MM-DD
    """
    try:
        # Validate date format
        datetime.strptime(start_date, "%Y-%m-%d")
        datetime.strptime(end_date, "%Y-%m-%d")

        # Convert to ISO format for API
        start_datetime = f"{start_date}T00:00:00Z"
        end_datetime = f"{end_date}T23:59:59Z"

        # Fetch sales and traffic data
        sales_traffic_data = get_amazon_sales_traffic_data(
            start_datetime, end_datetime, db, marketplace_ids
        )
        # Insert into database
        inserted_count = await insert_data_to_db(
            SALES_COLLECTION, sales_traffic_data, db
        )

        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "Sales and traffic data synced successfully",
                "records_inserted": inserted_count,
                "date_range": f"{start_date} to {end_date}",
                "marketplace_ids": marketplace_ids or [MARKETPLACE_ID],
            },
        )

    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid date format. Use YYYY-MM-DD",
        )
    except Exception as e:
        logger.error(f"Error syncing sales and traffic data: {e}")
        raise


@router.post("/sync/ledger")
async def sync_ledger_data(
    start_date: str,
    end_date: str,
    marketplace_ids: Optional[List[str]] = None,
    aggregate_by_location: str = "FC",
    aggregated_by_time_period: str = "DAILY",
    db=Depends(get_database),
):
    """
    Fetch ledger summary data from Amazon and insert into database
    Format: YYYY-MM-DD
    """
    try:
        # Validate date format
        datetime.strptime(start_date, "%Y-%m-%d")
        datetime.strptime(end_date, "%Y-%m-%d")

        # Convert to ISO format for API
        start_datetime = f"{start_date}T00:00:00Z"
        end_datetime = f"{end_date}T00:00:00Z"
        # Fetch ledger data
        ledger_data = get_amazon_ledger_data(
            start_datetime,
            end_datetime,
            db,
            marketplace_ids,
            aggregate_by_location,
            aggregated_by_time_period,
        )

        # Insert into database
        inserted_count = await insert_data_to_db(INVENTORY_COLLECTION, ledger_data, db)

        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "Ledger data synced successfully",
                "records_inserted": inserted_count,
                "date_range": f"{start_date} to {end_date}",
                "marketplace_ids": marketplace_ids or [MARKETPLACE_ID],
                "aggregate_by_location": aggregate_by_location,
                "aggregated_by_time_period": aggregated_by_time_period,
            },
        )

    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid date format. Use YYYY-MM-DD",
        )
    except Exception as e:
        logger.error(f"Error syncing ledger data: {e}")
        raise


@router.get("/status")
async def sync_status(
    start_date: str, end_date: str, report_type: str, db=Depends(get_database)
):
    try:
        # Parse date range
        start = datetime.strptime(start_date, "%Y-%m-%d").replace(
            hour=0, minute=0, second=0, microsecond=0, tzinfo=timezone.utc
        )
        end = datetime.strptime(end_date, "%Y-%m-%d").replace(
            hour=23, minute=59, second=59, microsecond=999999, tzinfo=timezone.utc
        )

        async def get_data_status(sales_collection, inventory_collection):
            """Helper function to get status for a collection pair"""
            # Get first and last sales dates in the date range
            sales_date_pipeline = [
                {"$match": {"date": {"$gte": start, "$lte": end}}},
                {
                    "$group": {
                        "_id": None,
                        "first_date": {"$min": "$date"},
                        "last_date": {"$max": "$date"},
                    }
                },
            ]

            sales_dates = list(db[sales_collection].aggregate(sales_date_pipeline))
            first_sales_date = None
            last_sales_date = None
            if sales_dates:
                first_sales_date = (
                    sales_dates[0]["first_date"].isoformat()
                    if sales_dates[0]["first_date"]
                    else None
                )
                last_sales_date = (
                    sales_dates[0]["last_date"].isoformat()
                    if sales_dates[0]["last_date"]
                    else None
                )

            # Get first and last inventory dates in the date range
            inventory_date_pipeline = [
                {"$match": {"date": {"$gte": start, "$lte": end}}},
                {
                    "$group": {
                        "_id": None,
                        "first_date": {"$min": "$date"},
                        "last_date": {"$max": "$date"},
                    }
                },
            ]

            inventory_dates = list(
                db[inventory_collection].aggregate(inventory_date_pipeline)
            )
            first_inventory_date = None
            last_inventory_date = None
            if inventory_dates:
                first_inventory_date = (
                    inventory_dates[0]["first_date"].isoformat()
                    if inventory_dates[0]["first_date"]
                    else None
                )
                last_inventory_date = (
                    inventory_dates[0]["last_date"].isoformat()
                    if inventory_dates[0]["last_date"]
                    else None
                )

            # Get total counts
            sales_count = db[sales_collection].count_documents(
                {"date": {"$gte": start, "$lte": end}}
            )
            inventory_count = db[inventory_collection].count_documents(
                {"date": {"$gte": start, "$lte": end}}
            )

            return {
                "sales_data": {
                    "records_count": sales_count,
                    "first_sales_date": first_sales_date,
                    "last_sales_date": last_sales_date,
                },
                "inventory_data": {
                    "records_count": inventory_count,
                    "first_inventory_date": first_inventory_date,
                    "last_inventory_date": last_inventory_date,
                },
            }

        # Handle different report types
        if report_type == "all":
            # Get both vendor_central and fba/seller_flex data
            vendor_status = await get_data_status(
                "amazon_vendor_sales", "amazon_vendor_inventory"
            )
            fba_status = await get_data_status(SALES_COLLECTION, INVENTORY_COLLECTION)

            return JSONResponse(
                status_code=status.HTTP_200_OK,
                content={
                    "date_range": {"start_date": start_date, "end_date": end_date},
                    "vendor_central": vendor_status,
                    "fba_seller_flex": fba_status,
                    "report_type": report_type,
                },
            )

        elif report_type == "vendor_central":
            status_data = await get_data_status(
                "amazon_vendor_sales", "amazon_vendor_inventory"
            )
        else:
            # fba, seller_flex, fba+seller_flex
            status_data = await get_data_status(SALES_COLLECTION, INVENTORY_COLLECTION)

        # For single report types, return the original structure
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "date_range": {"start_date": start_date, "end_date": end_date},
                "sales_data": status_data["sales_data"],
                "inventory_data": status_data["inventory_data"],
                "report_type": report_type,
            },
        )

    except Exception as e:
        logger.error(f"Error getting sync status: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e)
        )


@router.post("/sync/fba-returns")
async def sync_fba_returns_data(
    start_date: str,
    end_date: str,
    marketplace_ids: Optional[List[str]] = None,
    db=Depends(get_database),
):
    """
    Fetch FBA returns data from Amazon and insert into database
    Format: YYYY-MM-DD
    """
    try:
        # Validate date format
        datetime.strptime(start_date, "%Y-%m-%d")
        datetime.strptime(end_date, "%Y-%m-%d")

        # Convert to ISO format for API
        start_datetime = f"{start_date}T00:00:00Z"
        end_datetime = f"{end_date}T23:59:59Z"

        # Fetch FBA returns data
        fba_returns_data = get_amazon_fba_returns_data(
            start_datetime, end_datetime, db, marketplace_ids
        )

        # Insert into database
        inserted_count = await insert_data_to_db(
            FBA_RETURNS_COLLECTION, fba_returns_data, db
        )

        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "FBA returns data synced successfully",
                "records_inserted": inserted_count,
                "date_range": f"{start_date} to {end_date}",
                "marketplace_ids": marketplace_ids or [MARKETPLACE_ID],
            },
        )

    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid date format. Use YYYY-MM-DD",
        )
    except Exception as e:
        logger.error(f"Error syncing FBA returns data: {e}")
        raise


@router.post("/sync/sc-returns")
async def sync_sc_returns_data(
    start_date: str,
    end_date: str,
    marketplace_ids: Optional[List[str]] = None,
    db=Depends(get_database),
):
    """
    Fetch Seller Central returns data from Amazon and insert into database
    Format: YYYY-MM-DD
    """
    try:
        # Validate date format
        datetime.strptime(start_date, "%Y-%m-%d")
        datetime.strptime(end_date, "%Y-%m-%d")

        # Convert to ISO format for API
        start_datetime = f"{start_date}T00:00:00Z"
        end_datetime = f"{end_date}T23:59:59Z"

        # Fetch SC returns data
        sc_returns_data = get_amazon_sc_returns_data(
            start_datetime, end_datetime, db, marketplace_ids
        )

        # Insert into database
        inserted_count = await insert_data_to_db(
            SC_RETURNS_COLLECTION, sc_returns_data, db
        )

        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "Seller Central returns data synced successfully",
                "records_inserted": inserted_count,
                "date_range": f"{start_date} to {end_date}",
                "marketplace_ids": marketplace_ids or [MARKETPLACE_ID],
            },
        )

    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid date format. Use YYYY-MM-DD",
        )
    except Exception as e:
        logger.error(f"Error syncing SC returns data: {e}")
        raise


@router.post("/sync/daily-returns")
async def sync_daily_returns_data(
    db=Depends(get_database),
):
    """
    Fetch both FBA and SC returns data for a specific date (default: yesterday)
    This endpoint is designed for daily cron jobs
    Format: YYYY-MM-DD
    """
    try:
        start_date = datetime.now().replace(day=1).strftime("%Y-%m-%d")
        end_date = datetime.now().strftime("%Y-%m-%d")

        # Use the same date for start and end to get that specific day's data
        start_datetime = f"{start_date}T00:00:00Z"
        end_datetime = f"{end_date}T23:59:59Z"

        # Fetch both types of returns data
        fba_returns_data = get_amazon_fba_returns_data(start_datetime, end_datetime, db)

        sc_returns_data = get_amazon_sc_returns_data(start_datetime, end_datetime, db)

        # Insert into respective databases
        fba_inserted_count = await insert_data_to_db(
            FBA_RETURNS_COLLECTION, fba_returns_data, db
        )

        sc_inserted_count = await insert_data_to_db(
            SC_RETURNS_COLLECTION, sc_returns_data, db
        )

        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": f"Daily returns data synced successfully for {start_date}",
                "fba_returns": {
                    "records_inserted": fba_inserted_count,
                },
                "sc_returns": {
                    "records_inserted": sc_inserted_count,
                },
                "start_date": start_date,
                "end_date": end_date,
                "marketplace_ids": [MARKETPLACE_ID],
            },
        )

    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid date format. Use YYYY-MM-DD",
        )
    except Exception as e:
        logger.error(f"Error syncing daily returns data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to sync daily returns data: {str(e)}",
        )


@router.post("/sync/monthly-returns")
async def sync_monthly_returns_data(
    year: int,
    month: int,
    marketplace_ids: Optional[List[str]] = None,
    db=Depends(get_database),
):
    """
    Fetch both FBA and SC returns data for the entire month till current date
    """
    try:
        # Calculate date range for the month
        from calendar import monthrange

        # Get first day of the month
        start_date = datetime(year, month, 1)

        # Get current date or last day of month, whichever is earlier
        current_date = datetime.now()
        last_day_of_month = monthrange(year, month)[1]

        if year == current_date.year and month == current_date.month:
            # Current month - sync till today
            end_date = current_date
        else:
            # Past month - sync entire month
            end_date = datetime(year, month, last_day_of_month)

        # Format dates for API
        start_date_str = start_date.strftime("%Y-%m-%d")
        end_date_str = end_date.strftime("%Y-%m-%d")
        start_datetime = start_date.strftime("%Y-%m-%dT00:00:00Z")
        end_datetime = end_date.strftime("%Y-%m-%dT23:59:59Z")

        # Fetch both types of returns data
        fba_returns_data = get_amazon_fba_returns_data(
            start_datetime, end_datetime, db, marketplace_ids
        )

        sc_returns_data = get_amazon_sc_returns_data(
            start_datetime, end_datetime, db, marketplace_ids
        )

        # Insert into respective databases
        fba_inserted_count = await insert_data_to_db(
            FBA_RETURNS_COLLECTION, fba_returns_data, db
        )

        sc_inserted_count = await insert_data_to_db(
            SC_RETURNS_COLLECTION, sc_returns_data, db
        )

        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "Monthly returns data synced successfully",
                "fba_returns": {
                    "records_inserted": fba_inserted_count,
                },
                "sc_returns": {
                    "records_inserted": sc_inserted_count,
                },
                "date_range": f"{start_date_str} to {end_date_str}",
                "month": f"{year}-{month:02d}",
                "marketplace_ids": marketplace_ids or [MARKETPLACE_ID],
            },
        )

    except Exception as e:
        logger.error(f"Error syncing monthly returns data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to sync monthly returns data: {str(e)}",
        )


@router.get("/sales-traffic/{asin}")
async def get_asin_sales_data(
    asin: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    db=Depends(get_database),
):
    """
    Get sales and traffic data for a specific ASIN
    """
    try:
        query = {"$or": [{"parentAsin": asin}, {"childAsin": asin}]}

        # Add date range filter if provided
        if start_date and end_date:
            query["date"] = {"$gte": start_date, "$lte": end_date}

        collection = db[SALES_COLLECTION]
        results = list(collection.find(query).sort("date", 1))

        # Serialize MongoDB documents
        serialized_results = [serialize_mongo_document(doc) for doc in results]

        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "asin": asin,
                "records_found": len(serialized_results),
                "data": serialized_results,
            },
        )

    except Exception as e:
        logger.error(f"Error fetching ASIN data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e)
        )


@router.get("/ledger/summary")
async def get_ledger_summary(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    transaction_type: Optional[str] = None,
    db=Depends(get_database),
):
    """
    Get ledger summary data with optional filters
    """
    try:
        query = {}

        # Add date range filter if provided
        if start_date and end_date:
            query["Date"] = {"$gte": start_date, "$lte": end_date}

        # Add transaction type filter if provided
        if transaction_type:
            query["Transaction Type"] = transaction_type

        collection = db[INVENTORY_COLLECTION]
        results = list(collection.find(query).sort("Date", 1))

        # Serialize MongoDB documents
        serialized_results = [serialize_mongo_document(doc) for doc in results]

        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "records_found": len(serialized_results),
                "filters": {
                    "start_date": start_date,
                    "end_date": end_date,
                    "transaction_type": transaction_type,
                },
                "data": serialized_results,
            },
        )

    except Exception as e:
        logger.error(f"Error fetching ledger summary: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e)
        )


@router.get("/get_amazon_sku_mapping")
async def get_sku_mapping(database=Depends(get_database)):
    """
    Retrieves all documents from the SKU mapping collection.
    """
    try:
        sku_collection = database.get_collection(SKU_COLLECTION)
        sku_documents = serialize_mongo_document(
            list(sku_collection.find({}).sort("item_name", -1))
        )

        return JSONResponse(content=sku_documents)

    except PyMongoError as e:  # Catch database-specific errors
        logger.info(f"Database error retrieving SKU mapping: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Database error retrieving SKU mapping: {e}",
        )
    except Exception as e:  # Catch any other unexpected errors
        logger.info(f"Unexpected error retrieving SKU mapping: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An unexpected error occurred: {e}",
        )


@router.post("/upload-sku-mapping")
async def upload_sku_mapping(
    file: UploadFile = File(...), database=Depends(get_database)
):
    """
    Uploads SKU mapping data from an Excel file (.xlsx).
    Clears existing mapping and inserts new data.
    Expected columns: 'ASIN', 'SKU', 'Item Name'.
    """
    if not file.filename.endswith((".xlsx", ".xls")):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid file type. Please upload an Excel file (.xlsx or .xls).",
        )

    try:
        file_content = await file.read()
        df = pd.read_excel(
            BytesIO(file_content), sheet_name="E-trade Inventory Tracker"
        )

        # Validate required columns
        required_cols = ["ASIN", "SKU", "Item Name"]
        if not all(col in df.columns for col in required_cols):
            missing = [col for col in required_cols if col not in df.columns]
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Missing required columns: {', '.join(missing)}",
            )

        sku_collection = database.get_collection(SKU_COLLECTION)

        # Optional: Clear existing mapping or implement upsert logic
        # Clearing is simpler if the Excel is the source of truth for the full mapping
        delete_result = sku_collection.delete_many({})
        logger.info(f"Deleted {delete_result.deleted_count} existing SKU mappings.")

        data_to_insert = []
        for _, row in df.iterrows():
            item_id = str(row["ASIN"]).strip() if pd.notna(row["ASIN"]) else None
            sku_code = str(row["SKU"]).strip() if pd.notna(row["SKU"]) else None
            item_name = (
                str(row["Item Name"]).strip() if pd.notna(row["Item Name"]) else None
            )

            if (
                item_id and sku_code and item_name
            ):  # Ensure essential fields are present
                data_to_insert.append(
                    {
                        "item_id": item_id,
                        "sku_code": sku_code,
                        "item_name": item_name,
                    }
                )
            else:
                logger.info(f"Skipping SKU row due to missing data: {row.to_dict()}")

        if not data_to_insert:
            return {"message": "No valid SKU mapping data found to insert."}

        insert_result = sku_collection.insert_many(data_to_insert)
        logger.info(f"Inserted {len(insert_result.inserted_ids)} new SKU mappings.")

        # Create index for efficient lookup
        sku_collection.create_index([("item_id", ASCENDING)], unique=True)

        return {
            "message": f"Successfully uploaded and stored {len(insert_result.inserted_ids)} SKU mappings."
        }

    except HTTPException as e:
        raise e  # Re-raise intended HTTP exceptions
    except Exception as e:
        logger.info(f"Error uploading SKU mapping: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An error occurred processing the file: {e}",
        )


@router.post("/create_single_item")
async def create_single_item(body: dict):
    try:
        item_name = body.get("item_name")
        item_id = body.get("item_id")
        sku_code = body.get("sku_code")
        db = get_database()
        result = db[SKU_COLLECTION].find_one(
            {"item_name": item_name, "item_id": item_id}
        )
        if result:
            raise HTTPException(
                status_code=400, detail="Item Name and Item Id already Exists"
            )
        else:
            db[SKU_COLLECTION].insert_one(
                {"item_name": item_name, "item_id": int(item_id), "sku_code": sku_code}
            )
            return "Item Created"
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.info(f"Error uploading sales data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An error occurred creating the item: {e}",
        )


@router.delete("/delete_item/{item_id}")
async def delete_item(item_id: str):
    try:
        db = get_database()
        result = db[SKU_COLLECTION].delete_one({"_id": ObjectId(item_id)})
        if result.deleted_count == 1:
            return JSONResponse(status_code=200, content="Item Deleted Successfully")

    except HTTPException as e:
        raise e
    except Exception as e:
        logger.info(f"Error uploading sales data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An error occurred processing the file: {e}",
        )


async def generate_report_by_date_range(
    start_date: str, end_date: str, database: Any, report_type: str = "fba+seller_flex"
):
    try:
        start = datetime.strptime(start_date, "%Y-%m-%d").replace(
            hour=0, minute=0, second=0, microsecond=0, tzinfo=timezone.utc
        )
        end = datetime.strptime(end_date, "%Y-%m-%d").replace(
            hour=23, minute=59, second=59, microsecond=999999, tzinfo=timezone.utc
        )

        # Handle the new "all" report type
        if report_type == "all":
            # Get vendor_central data
            vendor_data = await generate_vendor_central_data(start, end, database)

            # Get fba+seller_flex data
            fba_seller_flex_data = await generate_amazon_data(
                start, end, database, "fba+seller_flex"
            )

            # Combine the data
            combined_data = combine_report_data(vendor_data, fba_seller_flex_data)

            return combined_data

        elif report_type == "vendor_central":
            return await generate_vendor_central_data(start, end, database)

        else:
            # Handle fba, seller_flex, fba+seller_flex
            return await generate_amazon_data(start, end, database, report_type)

    except Exception as e:
        import traceback

        logger.info(str(e))
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An error occurred generating Amazon report: {str(e)}",
        )


async def generate_vendor_central_data(start, end, database):
    """Generate vendor central report data"""
    vendor_pipeline = [
        {
            "$match": {
                "date": {"$gte": start, "$lte": end},
            }
        },
        {
            "$group": {
                "_id": {"asin": "$asin", "date": "$date"},
                "units_sold": {"$sum": "$orderedUnits"},
                "amount": {"$sum": "$orderedRevenue.amount"},
                # Handle missing customerReturns field with $ifNull
                "customer_returns": {"$sum": {"$ifNull": ["$customerReturns", 0]}},
            }
        },
        {
            "$lookup": {
                "from": "amazon_vendor_inventory",
                "let": {"sales_asin": "$_id.asin", "sales_date": "$_id.date"},
                "pipeline": [
                    {
                        "$match": {
                            "$expr": {
                                "$and": [
                                    {"$eq": ["$asin", "$$sales_asin"]},
                                    {"$eq": ["$date", "$$sales_date"]},
                                ]
                            }
                        }
                    },
                    {
                        "$group": {
                            "_id": {"asin": "$asin", "date": "$date"},
                            "closing_stock": {"$sum": "$sellableOnHandInventoryUnits"},
                        }
                    },
                ],
                "as": "inventory_data",
            }
        },
        {"$addFields": {"inventory_data": {"$last": "$inventory_data"}}},
        {
            "$lookup": {
                "from": "amazon_sku_mapping",
                "localField": "_id.asin",
                "foreignField": "item_id",
                "as": "item_info",
            }
        },
        {"$addFields": {"item_info": {"$last": "$item_info"}}},
        {"$sort": {"_id.date": -1}},
        {
            "$group": {
                "_id": {"asin": "$_id.asin"},
                "total_units_sold": {"$sum": "$units_sold"},
                "total_amount": {"$sum": "$amount"},
                "total_returns": {"$sum": "$customer_returns"},
                "last_day_closing_stock": {
                    "$first": {"$ifNull": ["$inventory_data.closing_stock", 0]}
                },
                "item_name": {"$last": "$item_info.item_name"},
                "sku_code": {"$last": "$item_info.sku_code"},
                "daily_data": {
                    "$push": {
                        "date": "$_id.date",
                        "closing_stock": {
                            "$ifNull": ["$inventory_data.closing_stock", 0]
                        },
                        "units_sold": "$units_sold",
                        "returns": "$customer_returns",  # ADD THIS for debugging
                    }
                },
            }
        },
        {
            "$addFields": {
                "total_days_in_stock": {
                    "$cond": {
                        "if": {
                            "$and": [
                                {"$ne": ["$daily_data", None]},
                                {"$gt": [{"$size": "$daily_data"}, 0]},
                            ]
                        },
                        "then": {
                            "$let": {
                                "vars": {
                                    "sorted_data": {
                                        "$sortArray": {
                                            "input": "$daily_data",
                                            "sortBy": {"date": 1},
                                        }
                                    }
                                },
                                "in": {
                                    "$size": {
                                        "$filter": {
                                            "input": "$$sorted_data",
                                            "cond": {
                                                "$gt": [
                                                    "$$this.closing_stock",
                                                    0,
                                                ]
                                            },
                                        }
                                    }
                                },
                            }
                        },
                        "else": 0,
                    }
                }
            }
        },
        {
            "$addFields": {
                "drr": {
                    "$cond": {
                        "if": {"$gt": ["$total_days_in_stock", 0]},
                        "then": {
                            "$divide": [
                                "$total_units_sold",
                                "$total_days_in_stock",
                            ]
                        },
                        "else": 0,
                    }
                }
            }
        },
        {
            "$project": {
                "_id": 0,
                "asin": "$_id.asin",
                "item_name": "$item_name",
                "sku_code": "$sku_code",
                "units_sold": "$total_units_sold",
                "closing_stock": "$last_day_closing_stock",
                "total_amount": {"$round": ["$total_amount", 2]},
                "stock": "$last_day_closing_stock",
                "total_days_in_stock": "$total_days_in_stock",
                "drr": {"$round": ["$drr", 2]},
                "total_returns": {
                    "$ifNull": ["$total_returns", 0]
                },  # Handle null returns
                "data_source": {"$literal": "vendor_central"},
            }
        },
        {
            "$sort": {
                "sku_code": -1,
                "asin": -1,
            }
        },
    ]

    collection = database.get_collection("amazon_vendor_sales")
    cursor = list(collection.aggregate(vendor_pipeline))
    result = serialize_mongo_document(cursor)
    return result


async def generate_amazon_data(start, end, database, report_type):
    """Generate FBA/Seller Flex report data with returns information (FBA only)"""
    ledger_match_conditions = [
        {"$eq": ["$asin", "$$sales_asin"]},
        {"$eq": ["$date", "$$sales_date"]},
        {"$eq": ["$disposition", "SELLABLE"]},
    ]

    if report_type == "fba":
        ledger_match_conditions.append({"$ne": ["$location", "VKSX"]})
    elif report_type == "seller_flex":
        ledger_match_conditions.append({"$eq": ["$location", "VKSX"]})

    base_pipeline = [
        {
            "$match": {
                "date": {"$gte": start, "$lte": end},
            }
        },
        {
            "$group": {
                "_id": {"asin": "$parentAsin", "date": "$date"},
                "units_sold": {"$sum": "$salesByAsin.unitsOrdered"},
                "amount": {"$sum": "$salesByAsin.orderedProductSales.amount"},
                "sessions": {
                    "$sum": {
                        "$add": [
                            {"$ifNull": ["$trafficByAsin.sessions", 0]},
                            {"$ifNull": ["$trafficByAsin.sessionsB2B", 0]},
                        ]
                    }
                },
            }
        },
        {
            "$lookup": {
                "from": "amazon_ledger",
                "let": {"sales_asin": "$_id.asin", "sales_date": "$_id.date"},
                "pipeline": [
                    {"$match": {"$expr": {"$and": ledger_match_conditions}}},
                    {
                        "$group": {
                            "_id": {"asin": "$asin", "date": "$date"},
                            "closing_stock": {"$sum": "$ending_warehouse_balance"},
                            "item_name": {"$last": "$title"},
                            "warehouses": {"$addToSet": "$location"},
                        }
                    },
                ],
                "as": "ledger_data",
            }
        },
        {"$addFields": {"ledger_data": {"$first": "$ledger_data"}}},
        {
            "$lookup": {
                "from": "amazon_sku_mapping",
                "localField": "_id.asin",
                "foreignField": "item_id",
                "as": "item_info",
            }
        },
        {"$addFields": {"item_info": {"$last": "$item_info"}}},
    ]

    # Add FBA returns lookup for FBA reports (not seller_flex)
    if report_type in ["fba", "fba+seller_flex", "all"]:
        fba_returns_lookup_stage = {
            "$lookup": {
                "from": FBA_RETURNS_COLLECTION,
                "let": {"sales_asin": "$_id.asin"},
                "pipeline": [
                    {
                        "$match": {
                            "$expr": {
                                "$and": [
                                    {"$eq": ["$asin", "$$sales_asin"]},
                                    {"$gte": ["$return_date", start]},
                                    {"$lte": ["$return_date", end]},
                                ]
                            }
                        }
                    },
                    {
                        "$group": {
                            "_id": "$asin",
                            "fba_returns": {
                                "$sum": {"$toInt": {"$ifNull": ["$quantity", 1]}}
                            },
                        }
                    },
                ],
                "as": "fba_returns_data",
            }
        }
        base_pipeline.append(fba_returns_lookup_stage)
        base_pipeline.append(
            {"$addFields": {"fba_returns_data": {"$first": "$fba_returns_data"}}}
        )

    # Add SC returns lookup for all report types
    sc_returns_lookup_stage = {
        "$lookup": {
            "from": SC_RETURNS_COLLECTION,
            "let": {"sales_asin": "$_id.asin"},
            "pipeline": [
                {
                    "$match": {
                        "$expr": {
                            "$and": [
                                {"$eq": ["$asin", "$$sales_asin"]},
                                {"$gte": ["$order_date", start]},
                                {"$lte": ["$order_date", end]},
                            ]
                        }
                    }
                },
                {
                    "$group": {
                        "_id": "$asin",
                        "sc_returns": {
                            "$sum": {"$toInt": {"$ifNull": ["$return_quantity", 1]}}
                        },
                    }
                },
            ],
            "as": "sc_returns_data",
        }
    }
    base_pipeline.append(sc_returns_lookup_stage)
    base_pipeline.append(
        {"$addFields": {"sc_returns_data": {"$first": "$sc_returns_data"}}}
    )

    # Continue with the rest of the pipeline
    group_stage = {
        "$group": {
            "_id": {"asin": "$_id.asin"},
            "total_units_sold": {"$sum": "$units_sold"},
            "total_amount": {"$sum": "$amount"},
            "total_sessions": {"$sum": "$sessions"},
            "total_closing_stock": {
                "$last": {"$ifNull": ["$ledger_data.closing_stock", 0]}
            },
            "item_name": {"$last": "$item_info.item_name"},
            "sku_code": {"$last": "$item_info.sku_code"},
            "all_warehouses": {"$addToSet": "$ledger_data.warehouses"},
            "daily_data": {
                "$push": {
                    "date": "$_id.date",
                    "closing_stock": {"$ifNull": ["$ledger_data.closing_stock", 0]},
                    "units_sold": "$units_sold",
                    "sessions": "$sessions",
                }
            },
        }
    }

    # Add combined returns field
    if report_type in ["fba", "fba+seller_flex"]:
        # FBA reports: FBA returns + SC returns
        group_stage["$group"]["total_returns"] = {
            "$last": {
                "$add": [
                    {"$ifNull": ["$fba_returns_data.fba_returns", 0]},
                    {"$ifNull": ["$sc_returns_data.sc_returns", 0]},
                ]
            }
        }
    elif report_type == "seller_flex":
        # Seller Flex reports: Only SC returns (no FBA returns)
        group_stage["$group"]["total_returns"] = {
            "$last": {"$ifNull": ["$sc_returns_data.sc_returns", 0]}
        }

    base_pipeline.append(group_stage)

    # Add remaining pipeline stages
    base_pipeline.extend(
        [
            {
                "$addFields": {
                    "total_days_in_stock": {
                        "$cond": {
                            "if": {
                                "$and": [
                                    {"$ne": ["$daily_data", None]},
                                    {"$gt": [{"$size": "$daily_data"}, 0]},
                                ]
                            },
                            "then": {
                                "$let": {
                                    "vars": {
                                        "sorted_data": {
                                            "$sortArray": {
                                                "input": "$daily_data",
                                                "sortBy": {"date": 1},
                                            }
                                        }
                                    },
                                    "in": {
                                        "$size": {
                                            "$filter": {
                                                "input": "$$sorted_data",
                                                "cond": {
                                                    "$gt": ["$$this.closing_stock", 0]
                                                },
                                            }
                                        }
                                    },
                                }
                            },
                            "else": 0,
                        }
                    },
                    "warehouses": {
                        "$reduce": {
                            "input": "$all_warehouses",
                            "initialValue": [],
                            "in": {"$setUnion": ["$$value", "$$this"]},
                        }
                    },
                }
            },
            {
                "$addFields": {
                    "drr": {
                        "$cond": {
                            "if": {"$gt": ["$total_days_in_stock", 0]},
                            "then": {
                                "$divide": ["$total_units_sold", "$total_days_in_stock"]
                            },
                            "else": 0,
                        }
                    }
                }
            },
        ]
    )

    # Build project stage dynamically based on report type
    project_stage = {
        "$project": {
            "_id": 0,
            "asin": "$_id.asin",
            "sku_code": "$sku_code",
            "item_name": "$item_name",
            "warehouses": "$warehouses",
            "units_sold": "$total_units_sold",
            "total_amount": "$total_amount",
            "sessions": "$total_sessions",
            "closing_stock": "$total_closing_stock",
            "total_returns": "$total_returns",  # Always include total_returns
            "total_days_in_stock": "$total_days_in_stock",
            "drr": {"$round": ["$drr", 2]},
            "data_source": {"$literal": report_type},
        }
    }

    base_pipeline.extend(
        [
            project_stage,
            {
                "$sort": {
                    "sku_code": -1,
                    "asin": -1,
                }
            },
        ]
    )

    collection = database.get_collection(SALES_COLLECTION)
    cursor = list(collection.aggregate(base_pipeline))
    result = serialize_mongo_document(cursor)
    return result


def combine_report_data(vendor_data, fba_seller_flex_data):
    """Combine vendor central and fba/seller flex data"""
    # Create lookup dictionaries
    vendor_lookup = {}
    fba_lookup = {}

    # Index vendor data by sku_code and asin
    for item in vendor_data:
        key = (item.get("sku_code"), item.get("asin"))
        vendor_lookup[key] = item

    # Index fba data by sku_code and asin
    for item in fba_seller_flex_data:
        key = (item.get("sku_code"), item.get("asin"))
        fba_lookup[key] = item

    combined_results = []
    processed_keys = set()

    # Combine matching records
    for key in vendor_lookup:
        if key in fba_lookup:
            vendor_item = vendor_lookup[key]
            fba_item = fba_lookup[key]

            combined_item = {
                "asin": vendor_item.get("asin") or fba_item.get("asin"),
                "sku_code": vendor_item.get("sku_code") or fba_item.get("sku_code"),
                "item_name": vendor_item.get("item_name") or fba_item.get("item_name"),
                "warehouses": fba_item.get("warehouses", []),  # Only from FBA data
                "units_sold": (
                    vendor_item.get("units_sold", 0) + fba_item.get("units_sold", 0)
                ),
                "total_amount": round(
                    (
                        vendor_item.get("total_amount", 0)
                        + fba_item.get("total_amount", 0)
                    ),
                    2,
                ),
                "sessions": fba_item.get("sessions", 0),  # Only from FBA data
                "closing_stock": (
                    vendor_item.get("closing_stock", 0)
                    + fba_item.get("closing_stock", 0)
                ),
                "stock": (
                    vendor_item.get("stock", 0) + fba_item.get("closing_stock", 0)
                ),  # Combined stock
                "total_days_in_stock": max(
                    vendor_item.get("total_days_in_stock", 0),
                    fba_item.get("total_days_in_stock", 0),
                ),
                "total_returns": (
                    vendor_item.get("total_returns", 0)
                    + fba_item.get("total_returns", 0)
                ),
                "drr": 0,  # Will be recalculated
                "data_source": "combined",
            }

            # Recalculate DRR
            if combined_item["total_days_in_stock"] > 0:
                combined_item["drr"] = round(
                    combined_item["units_sold"] / combined_item["total_days_in_stock"],
                    2,
                )

            combined_results.append(combined_item)
            processed_keys.add(key)

    # Add vendor-only records
    for key, vendor_item in vendor_lookup.items():
        if key not in processed_keys:
            # Add missing fields with default values
            vendor_item["warehouses"] = []
            vendor_item["sessions"] = 0
            if "stock" not in vendor_item:
                vendor_item["stock"] = vendor_item.get("closing_stock", 0)
            if "total_returns" not in vendor_item:
                vendor_item["total_returns"] = 0
            vendor_item["data_source"] = "vendor_only"
            combined_results.append(vendor_item)

    # Add FBA-only records
    for key, fba_item in fba_lookup.items():
        if key not in processed_keys:
            # Add missing fields with default values
            fba_item["stock"] = fba_item.get("closing_stock", 0)
            fba_item["data_source"] = "fba_only"
            combined_results.append(fba_item)

    # Sort the results
    combined_results.sort(
        key=lambda x: (x.get("sku_code") or "", x.get("asin") or ""), reverse=True
    )

    return combined_results


@router.get("/get_report_data_by_date_range")
async def get_report_data_by_date_range(
    start_date: str,
    end_date: str,
    report_type: str = "fba+seller_flex",
    database=Depends(get_database),
):
    try:
        # Updated valid types to include "all"
        valid_types = ["fba+seller_flex", "fba", "seller_flex", "vendor_central", "all"]
        if report_type not in valid_types:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid type. Must be one of: {', '.join(valid_types)}",
            )

        report_response = await generate_report_by_date_range(
            start_date=start_date,
            end_date=end_date,
            database=database,
            report_type=report_type,
        )
        logger.info(f"Retrieved dynamic report data of type: {report_type}")
        return report_response

    except HTTPException as e:
        raise e
    except Exception as e:
        logger.info(f"Error retrieving report data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An error occurred retrieving the report data: {e}",
        )


def format_column_name(column_name):
    """Convert snake_case column names to proper formatted names"""
    # Replace underscores with spaces and title case
    formatted = column_name.replace("_", " ").title()

    # Handle specific cases for better formatting
    replacements = {
        "Asin": "ASIN",
        "Sku Code": "SKU Code",
        "Item Name": "Item Name",
        "Units Sold": "Units Sold",
        "Total Amount": "Total Amount",
        "Closing Stock": "Closing Stock",
        "Sessions": "Sessions",
        "Warehouses": "Warehouses",
    }

    return replacements.get(formatted, formatted)


@router.get("/download_report_by_date_range")
async def download_report_by_date_range(
    start_date: str,
    end_date: str,
    report_type: str = "fba+seller_flex",  # "fba+seller_flex", "fba", "seller_flex", "vendor_central", or "all"
    database=Depends(get_database),
):

    try:
        # Validate report_type parameter - Updated to include "all"
        valid_types = ["fba+seller_flex", "fba", "seller_flex", "vendor_central", "all"]
        if report_type not in valid_types:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid report_type. Must be one of: {', '.join(valid_types)}",
            )

        # Generate the report data using existing function
        report_data = await generate_report_by_date_range(
            start_date=start_date,
            end_date=end_date,
            database=database,
            report_type=report_type,
        )

        if not report_data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="No data found for the specified date range and report type",
            )

        # Convert to DataFrame
        df = pd.DataFrame(report_data)

        # Format column names
        column_mapping = {}
        for col in df.columns:
            column_mapping[col] = format_column_name(col)

        df = df.rename(columns=column_mapping)

        # Handle the warehouses column (convert list to string) - only for non-vendor reports
        if "Warehouses" in df.columns:
            df["Warehouses"] = df["Warehouses"].apply(
                lambda x: ", ".join(x) if isinstance(x, list) else str(x)
            )

        # Reorder columns for better presentation
        # Different column orders for different report types
        if report_type == "vendor_central":
            preferred_order = [
                "ASIN",
                "SKU Code",
                "Item Name",
                "Units Sold",
                "Total Amount",
                "Closing Stock",
                "Stock",  # vendor_central includes both closing_stock and stock
                "Total Days In Stock",
                "Drr",
            ]
        elif report_type == "all":
            # Combined report includes all fields from both vendor and FBA data
            preferred_order = [
                "ASIN",
                "SKU Code",
                "Item Name",
                "Warehouses",
                "Units Sold",
                "Total Amount",
                "Sessions",
                "Closing Stock",
                "Stock",  # Combined reports include both closing_stock and stock
                "Total Returns",
                "Total Days In Stock",
                "Drr",
                "Data Source",  # Show which source the data came from
            ]
        else:
            # FBA, Seller Flex, and FBA+Seller Flex reports
            preferred_order = [
                "ASIN",
                "SKU Code",
                "Item Name",
                "Warehouses",
                "Units Sold",
                "Total Amount",
                "Sessions",
                "Closing Stock",
                "Total Returns",
                "Total Days In Stock",
                "Drr",
            ]

        # Only include columns that exist in the DataFrame
        column_order = [col for col in preferred_order if col in df.columns]
        # Add any remaining columns not in preferred order
        remaining_cols = [col for col in df.columns if col not in column_order]
        final_column_order = column_order + remaining_cols

        df = df[final_column_order]

        # Create Excel file in memory
        excel_buffer = io.BytesIO()

        with pd.ExcelWriter(excel_buffer, engine="openpyxl") as writer:
            df.to_excel(writer, sheet_name="Report Data", index=False)

            # Get the workbook and worksheet for formatting
            workbook = writer.book
            worksheet = writer.sheets["Report Data"]

            # Auto-adjust column widths
            for column in worksheet.columns:
                max_length = 0
                column_letter = column[0].column_letter

                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass

                adjusted_width = min(max_length + 2, 50)  # Cap at 50 characters
                worksheet.column_dimensions[column_letter].width = adjusted_width

            # Style the header row
            from openpyxl.styles import Font, PatternFill

            header_font = Font(bold=True, color="FFFFFF")
            header_fill = PatternFill(
                start_color="366092", end_color="366092", fill_type="solid"
            )

            for cell in worksheet[1]:
                cell.font = header_font
                cell.fill = header_fill

            # Special formatting for "all" report type - color code rows by data source
            if report_type == "all" and "Data Source" in df.columns:
                from openpyxl.styles import PatternFill

                # Define colors for different data sources
                color_mapping = {
                    "combined": PatternFill(
                        start_color="E8F5E8", end_color="E8F5E8", fill_type="solid"
                    ),  # Light green
                    "vendor_only": PatternFill(
                        start_color="FFF2CC", end_color="FFF2CC", fill_type="solid"
                    ),  # Light yellow
                    "fba_only": PatternFill(
                        start_color="E1F5FE", end_color="E1F5FE", fill_type="solid"
                    ),  # Light blue
                }

                # Apply color coding to rows based on data source
                data_source_col_index = (
                    df.columns.get_loc("Data Source") + 1
                )  # +1 because Excel is 1-indexed

                for row_num in range(2, len(df) + 2):  # Start from row 2 (after header)
                    data_source_value = worksheet.cell(
                        row=row_num, column=data_source_col_index
                    ).value
                    row_fill = color_mapping.get(data_source_value)

                    if row_fill:
                        for col_num in range(1, len(df.columns) + 1):
                            worksheet.cell(row=row_num, column=col_num).fill = row_fill

        excel_buffer.seek(0)

        # Generate filename with timestamp and report type
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        type_suffix = f"_{report_type}" if report_type != "fba+seller_flex" else ""
        filename = (
            f"inventory_report_{start_date}_to_{end_date}{type_suffix}_{timestamp}.xlsx"
        )

        # Create streaming response
        response = StreamingResponse(
            io.BytesIO(excel_buffer.read()),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename={filename}"},
        )

        logger.info(
            f"Generated Excel report for report_type: {report_type}, rows: {len(df)}"
        )
        return response

    except HTTPException as e:
        logger.info(str(e))
        raise e
    except Exception as e:
        logger.info(f"Error generating Excel report: {e}")
        logger.info(str(e))
        import traceback

        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An error occurred generating the Excel report: {e}",
        )


# Updated format_column_name function to handle the new "Data Source" column
def format_column_name(column_name):
    """Convert snake_case column names to proper formatted names"""
    # Replace underscores with spaces and title case
    formatted = column_name.replace("_", " ").title()

    # Handle specific cases for better formatting
    replacements = {
        "Asin": "ASIN",
        "Sku Code": "SKU Code",
        "Item Name": "Item Name",
        "Units Sold": "Units Sold",
        "Total Amount": "Total Amount",
        "Closing Stock": "Closing Stock",
        "Sessions": "Sessions",
        "Warehouses": "Warehouses",
        "Data Source": "Data Source",
        "Total Days In Stock": "Total Days In Stock",
        "Drr": "DRR",
        "Total Returns": "Total Returns",
        "Stock": "Stock",
    }

    return replacements.get(formatted, formatted)


@router.post("/sync/settlements")
async def sync_settlement_reports(
    days_back: Optional[int] = 7,
    db=Depends(get_database),
):
    try:
        logger.info(f"Starting settlement reports sync for last {days_back} days")
        from io import StringIO
        import pandas as pd

        marketplace_endpoints = {
            "A21TJRUUN4KGV": "https://sellingpartnerapi-eu.amazon.com",
            "A1F83G8C2ARO7P": "https://sellingpartnerapi-eu.amazon.com",
            "A1PA6795UKMFR9": "https://sellingpartnerapi-eu.amazon.com",
            "ATVPDKIKX0DER": "https://sellingpartnerapi-na.amazon.com",
            "A2EUQ1WTGCTBG2": "https://sellingpartnerapi-na.amazon.com",
            "A1VC38T7YXB528": "https://sellingpartnerapi-fe.amazon.com",
        }

        base_url = marketplace_endpoints.get(
            MARKETPLACE_ID, "https://sellingpartnerapi-eu.amazon.com"
        )

        # Get access token
        access_token = get_amazon_token()

        # Create compound index for faster duplicate checking
        collection = db["amazon_settlements"]

        # Get settlement reports
        reports_url = f"{base_url}/reports/2021-06-30/reports"
        headers = {"x-amz-access-token": access_token}

        created_since = datetime.now() - timedelta(days=days_back)

        params = {
            "reportTypes": "GET_V2_SETTLEMENT_REPORT_DATA_FLAT_FILE_V2",
            "createdSince": created_since.strftime("%Y-%m-%dT%H:%M:%SZ"),
            "pageSize": 100,
        }

        response = requests.get(reports_url, headers=headers, params=params)
        response.raise_for_status()

        reports = response.json().get("reports", [])

        if not reports:
            logger.info("No settlement reports found")
            return JSONResponse(
                status_code=status.HTTP_200_OK,
                content={
                    "message": "No settlement reports found",
                    "reports_processed": 0,
                    "records_inserted": 0,
                    "records_skipped": 0,
                },
            )

        # Filter completed reports
        done_reports = [r for r in reports if r.get("processingStatus") == "DONE"]

        logger.info(f"Found {len(done_reports)} completed settlement reports")

        total_inserted = 0
        total_skipped = 0
        reports_processed = 0

        # Process each report
        for report in done_reports:
            report_id = report.get("reportId")
            report_date = report.get("dataStartTime", "")[:10]

            # Check if already processed
            existing_count = collection.count_documents({"report_id": report_id})
            if existing_count > 0:
                logger.info(
                    f"Report {report_id} already processed ({existing_count} records)"
                )
                continue

            try:
                # Get report document ID
                report_url = f"{base_url}/reports/2021-06-30/reports/{report_id}"
                report_response = requests.get(report_url, headers=headers)
                report_response.raise_for_status()

                report_data = report_response.json()
                document_id = report_data.get("reportDocumentId")

                if not document_id:
                    logger.warning(f"No document ID for report {report_id}")
                    continue

                # Get document download URL
                doc_url = f"{base_url}/reports/2021-06-30/documents/{document_id}"
                doc_response = requests.get(doc_url, headers=headers)
                doc_response.raise_for_status()

                download_url = doc_response.json().get("url")

                # Download report data
                data_response = requests.get(download_url)
                data_response.raise_for_status()

                # Parse TSV data
                df = pd.read_csv(
                    StringIO(data_response.text),
                    sep="\t",
                    encoding="utf-8",
                    low_memory=False,
                )

                # Extract required fields
                required_fields = [
                    "order-id",
                    "amount-description",
                    "amount",
                    "sku",
                    "quantity-purchased",
                    "posted-date",
                ]

                existing_fields = [
                    field for field in required_fields if field in df.columns
                ]

                if not existing_fields:
                    logger.warning(f"No required fields found in report {report_id}")
                    continue

                extracted_df = df[existing_fields].copy()
                records = extracted_df.to_dict("records")

                # Process records
                processed_records = []

                for record in records:
                    new_record = {}

                    # Map fields
                    field_mapping = {
                        "order-id": "order_id",
                        "amount-description": "amount_description",
                        "amount": "amount",
                        "sku": "sku",
                        "quantity-purchased": "quantity_purchased",
                        "posted-date": "posted_date",
                    }

                    for old_key, new_key in field_mapping.items():
                        if old_key in record:
                            value = record[old_key]
                            if pd.isna(value) or value == "":
                                new_record[new_key] = None
                            else:
                                new_record[new_key] = value

                    # Skip if order_id is null
                    if not new_record.get("order_id"):
                        continue

                    # Add metadata
                    new_record["report_id"] = report_id
                    new_record["report_date"] = report_date
                    new_record["created_at"] = datetime.now()

                    # Handle data type conversions
                    if new_record.get("amount") is not None:
                        try:
                            new_record["amount"] = float(new_record["amount"])
                        except:
                            pass

                    if new_record.get("posted_date") is not None:
                        try:
                            new_record["posted_date"] = datetime.strptime(
                                new_record["posted_date"], "%d.%m.%Y"
                            )
                        except:
                            pass

                    try:
                        new_record["report_date"] = datetime.strptime(
                            report_date, "%Y-%m-%d"
                        )
                    except:
                        pass

                    if new_record.get("quantity_purchased") is not None:
                        try:
                            new_record["quantity_purchased"] = int(
                                new_record["quantity_purchased"]
                            )
                        except:
                            pass

                    processed_records.append(new_record)

                if not processed_records:
                    logger.warning(f"No valid records in report {report_id}")
                    continue

                # Check for duplicates efficiently
                filter_queries = []
                for record in processed_records:
                    filter_queries.append(
                        {
                            "order_id": record.get("order_id"),
                            "amount_description": record.get("amount_description"),
                            "amount": record.get("amount"),
                            "sku": record.get("sku"),
                            "quantity_purchased": record.get("quantity_purchased"),
                            "posted_date": record.get("posted_date"),
                            "report_id": record.get("report_id"),
                        }
                    )

                # Find existing records
                existing_records = list(
                    collection.find(
                        {"$or": filter_queries},
                        {
                            "order_id": 1,
                            "amount_description": 1,
                            "amount": 1,
                            "sku": 1,
                            "quantity_purchased": 1,
                            "posted_date": 1,
                            "report_id": 1,
                        },
                    )
                )

                # Create signature set
                existing_signatures = set()
                for existing in existing_records:
                    signature = (
                        existing.get("order_id"),
                        existing.get("amount_description"),
                        existing.get("amount"),
                        existing.get("sku"),
                        existing.get("quantity_purchased"),
                        existing.get("posted_date"),
                        existing.get("report_id"),
                    )
                    existing_signatures.add(signature)

                # Filter out duplicates
                records_to_insert = []
                duplicate_count = 0

                for record in processed_records:
                    signature = (
                        record.get("order_id"),
                        record.get("amount_description"),
                        record.get("amount"),
                        record.get("sku"),
                        record.get("quantity_purchased"),
                        record.get("posted_date"),
                        record.get("report_id"),
                    )

                    if signature not in existing_signatures:
                        records_to_insert.append(record)
                        existing_signatures.add(signature)
                    else:
                        duplicate_count += 1

                # Insert records
                if records_to_insert:
                    result = collection.insert_many(records_to_insert, ordered=False)
                    inserted = len(result.inserted_ids)
                    total_inserted += inserted
                    total_skipped += duplicate_count
                    reports_processed += 1

                    logger.info(
                        f"Report {report_id}: Inserted {inserted}, Skipped {duplicate_count}"
                    )
                else:
                    logger.info(f"Report {report_id}: All records already exist")

                # Small delay to avoid rate limiting
                time.sleep(0.5)

            except Exception as e:
                logger.error(f"Error processing report {report_id}: {e}")
                continue

        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "Settlement reports sync completed",
                "reports_processed": reports_processed,
                "records_inserted": total_inserted,
                "records_skipped": total_skipped,
                "days_back": days_back,
            },
        )

    except Exception as e:
        logger.error(f"Error syncing settlement reports: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to sync settlement reports: {str(e)}",
        )


@router.get("/settlements/pivot")
async def get_settlements_pivot(
    start_date: Optional[str] = Query(
        None, description="Start date in YYYY-MM-DD format"
    ),
    end_date: Optional[str] = Query(None, description="End date in YYYY-MM-DD format"),
    sku: Optional[str] = Query(None, description="Filter by SKU"),
    order_id: Optional[str] = Query(None, description="Filter by order ID"),
    format: str = Query("json", description="Response format: 'json' or 'excel'"),
    db=Depends(get_database),
):
    """Get Amazon settlements data in pivot table format."""
    try:
        # Build MongoDB query
        query = {}

        if start_date or end_date:
            query["posted_date"] = {}
            if start_date:
                try:
                    start_dt = datetime.strptime(start_date, "%Y-%m-%d")
                    query["posted_date"]["$gte"] = start_dt
                except ValueError:
                    raise HTTPException(
                        status_code=status.HTTP_400_BAD_REQUEST,
                        detail="Invalid start_date format. Use YYYY-MM-DD",
                    )
            if end_date:
                try:
                    end_dt = datetime.strptime(end_date, "%Y-%m-%d")
                    query["posted_date"]["$lte"] = end_dt
                except ValueError:
                    raise HTTPException(
                        status_code=status.HTTP_400_BAD_REQUEST,
                        detail="Invalid end_date format. Use YYYY-MM-DD",
                    )

        if sku:
            query["sku"] = sku

        if order_id:
            query["order_id"] = order_id

        # Fetch data from MongoDB
        collection = db["amazon_settlements"]
        cursor = collection.find(query)
        settlements = list(cursor)

        if not settlements:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="No settlement records found for the given criteria",
            )

        # Convert to DataFrame
        df = pd.DataFrame(settlements)
        df = df[
            [
                "order_id",
                "posted_date",
                "sku",
                "quantity_purchased",
                "amount_description",
                "amount",
            ]
        ]
        df["posted_date"] = pd.to_datetime(df["posted_date"]).dt.strftime("%Y-%m-%d")

        # Create pivot table
        pivot_df = df.pivot_table(
            index=["order_id", "posted_date", "sku", "quantity_purchased"],
            columns="amount_description",
            values="amount",
            aggfunc="sum",
            fill_value=None,
        )

        pivot_df = pivot_df.reset_index()

        # Calculate Grand Total (sum of all amount columns)
        amount_columns = [
            col
            for col in pivot_df.columns
            if col not in ["order_id", "posted_date", "sku", "quantity_purchased"]
        ]
        pivot_df["Grand Total"] = pivot_df[amount_columns].sum(axis=1)

        # 🔧 FIX: Replace NaN/inf values with None for JSON serialization
        pivot_df = pivot_df.replace([float("nan"), float("inf"), float("-inf")], None)

        # Sort by posted_date
        pivot_df = pivot_df.sort_values("posted_date", ascending=False)

        # Format response based on requested format
        if format.lower() == "excel":
            output = BytesIO()

            with pd.ExcelWriter(output, engine="openpyxl") as writer:
                pivot_df.to_excel(writer, sheet_name="Sheet1", index=False)

                workbook = writer.book
                worksheet = writer.sheets["Sheet1"]

                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column_letter].width = adjusted_width

            output.seek(0)

            filename = "amazon_settlements_pivot"
            if start_date:
                filename += f"_{start_date}"
            if end_date:
                filename += f"_to_{end_date}"
            filename += ".xlsx"

            return StreamingResponse(
                output,
                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                headers={"Content-Disposition": f"attachment; filename={filename}"},
            )

        else:
            # Return as JSON
            result = pivot_df.to_dict(orient="records")

            return {
                "success": True,
                "count": len(result),
                "filters": {
                    "start_date": start_date,
                    "end_date": end_date,
                    "sku": sku,
                    "order_id": order_id,
                },
                "data": result,
            }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating settlements pivot: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate settlements pivot: {str(e)}",
        )


@router.get("/settlements/pivot/columns")
async def get_pivot_columns(db=Depends(get_database)):
    """
    Get list of all unique amount_description values (column names in pivot table).

    This is useful for understanding what charge types exist in your data.
    """
    try:
        collection = db["amazon_settlements"]

        # Get distinct amount_description values
        columns = collection.distinct("amount_description")

        # Remove None values
        columns = [col for col in columns if col is not None]

        # Sort alphabetically
        columns.sort()

        return {"success": True, "count": len(columns), "columns": columns}

    except Exception as e:
        logger.error(f"Error fetching pivot columns: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch pivot columns: {str(e)}",
        )


@router.get("/settlements/summary")
async def get_settlements_summary(
    start_date: Optional[str] = Query(
        None, description="Start date in YYYY-MM-DD format"
    ),
    end_date: Optional[str] = Query(None, description="End date in YYYY-MM-DD format"),
    group_by: str = Query(
        "amount_description",
        description="Group by: 'amount_description', 'sku', or 'date'",
    ),
    db=Depends(get_database),
):
    """
    Get summary statistics for settlements data.

    Query Parameters:
    - start_date: Filter by posted date (start)
    - end_date: Filter by posted date (end)
    - group_by: How to group the summary - 'amount_description', 'sku', or 'date'

    Returns summary totals grouped by the specified field.
    """
    try:
        # Build match stage
        match_stage = {}

        if start_date or end_date:
            match_stage["posted_date"] = {}
            if start_date:
                start_dt = datetime.strptime(start_date, "%Y-%m-%d")
                match_stage["posted_date"]["$gte"] = start_dt
            if end_date:
                end_dt = datetime.strptime(end_date, "%Y-%m-%d")
                match_stage["posted_date"]["$lte"] = end_dt

        # Build aggregation pipeline
        collection = db["amazon_settlements"]

        group_field_map = {
            "amount_description": "$amount_description",
            "sku": "$sku",
            "date": {"$dateToString": {"format": "%Y-%m-%d", "date": "$posted_date"}},
        }

        if group_by not in group_field_map:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid group_by value. Must be one of: {', '.join(group_field_map.keys())}",
            )

        pipeline = []

        if match_stage:
            pipeline.append({"$match": match_stage})

        pipeline.extend(
            [
                {
                    "$group": {
                        "_id": group_field_map[group_by],
                        "total_amount": {"$sum": "$amount"},
                        "count": {"$sum": 1},
                        "avg_amount": {"$avg": "$amount"},
                    }
                },
                {"$sort": {"total_amount": -1}},
            ]
        )

        results = list(collection.aggregate(pipeline))

        # Format results
        formatted_results = []
        for result in results:
            formatted_results.append(
                {
                    group_by: result["_id"],
                    "total_amount": (
                        round(result["total_amount"], 2)
                        if result["total_amount"]
                        else 0
                    ),
                    "count": result["count"],
                    "average_amount": (
                        round(result["avg_amount"], 2) if result["avg_amount"] else 0
                    ),
                }
            )

        return {
            "success": True,
            "group_by": group_by,
            "filters": {"start_date": start_date, "end_date": end_date},
            "count": len(formatted_results),
            "data": formatted_results,
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating settlements summary: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate settlements summary: {str(e)}",
        )
